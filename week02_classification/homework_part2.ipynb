{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary prediction, episode II: make it actually work (4 points)\n",
    "\n",
    "Your main task is to use some of the tricks you've learned on the network and analyze if you can improve __validation MAE__. Try __at least 3 options__ from the list below for a passing grade. Write a short report about what you have tried. More ideas = more bonus points. \n",
    "\n",
    "__Please be serious:__ \" plot learning curves in MAE/epoch, compare models based on optimal performance, test one change at a time. You know the drill :)\n",
    "\n",
    "You can use either __pytorch__ or __tensorflow__ or any other framework (e.g. pure __keras__). Feel free to adapt the seminar code for your needs. For tensorflow version, consider `seminar_tf2.ipynb` as a starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12612628</td>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking, Surrey, Surrey</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12612830</td>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow, Scotland, Scotland</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 35000/annum 25-35K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12612844</td>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire, South East, South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 40000/annum 20-40K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12613049</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 30000/annum 25K-30K negotiable</td>\n",
       "      <td>27500</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12613647</td>\n",
       "      <td>Pioneer, Miser Engineering Systems Analyst</td>\n",
       "      <td>Pioneer, Miser  Engineering Systems Analyst Do...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  12612628                        Engineering Systems Analyst   \n",
       "1  12612830                            Stress Engineer Glasgow   \n",
       "2  12612844                   Modelling and simulation analyst   \n",
       "3  12613049  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  12613647         Pioneer, Miser Engineering Systems Analyst   \n",
       "\n",
       "                                     FullDescription  \\\n",
       "0  Engineering Systems Analyst Dorking Surrey Sal...   \n",
       "1  Stress Engineer Glasgow Salary **** to **** We...   \n",
       "2  Mathematical Modeller / Simulation Analyst / O...   \n",
       "3  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  Pioneer, Miser  Engineering Systems Analyst Do...   \n",
       "\n",
       "                         LocationRaw LocationNormalized ContractType  \\\n",
       "0            Dorking, Surrey, Surrey            Dorking          NaN   \n",
       "1        Glasgow, Scotland, Scotland            Glasgow          NaN   \n",
       "2  Hampshire, South East, South East          Hampshire          NaN   \n",
       "3     Surrey, South East, South East             Surrey          NaN   \n",
       "4     Surrey, South East, South East             Surrey          NaN   \n",
       "\n",
       "  ContractTime                       Company          Category  \\\n",
       "0    permanent  Gregory Martin International  Engineering Jobs   \n",
       "1    permanent  Gregory Martin International  Engineering Jobs   \n",
       "2    permanent  Gregory Martin International  Engineering Jobs   \n",
       "3    permanent  Gregory Martin International  Engineering Jobs   \n",
       "4    permanent  Gregory Martin International  Engineering Jobs   \n",
       "\n",
       "                                SalaryRaw  SalaryNormalized        SourceName  \n",
       "0              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  \n",
       "1              25000 - 35000/annum 25-35K             30000  cv-library.co.uk  \n",
       "2              20000 - 40000/annum 20-40K             30000  cv-library.co.uk  \n",
       "3  25000 - 30000/annum 25K-30K negotiable             27500  cv-library.co.uk  \n",
       "4              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./Train_rev1.zip\", compression='zip', index_col=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
    "\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data[\"SalaryNormalized\"], bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(data['Log1pSalary'], bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>Log1pSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15485</th>\n",
       "      <td>66750798</td>\n",
       "      <td>Accounts Assistant</td>\n",
       "      <td>Accounts Assistant **** **** Hereford A Herefo...</td>\n",
       "      <td>Hereford</td>\n",
       "      <td>Hereford</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Accounting &amp; Finance Jobs</td>\n",
       "      <td>15,000 - 20,000</td>\n",
       "      <td>17500</td>\n",
       "      <td>hays.co.uk</td>\n",
       "      <td>9.770013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241558</th>\n",
       "      <td>72678055</td>\n",
       "      <td>UK Country Manager / Sales Executive</td>\n",
       "      <td>Job Title UK Country Manager / Sales Executive...</td>\n",
       "      <td>South East England,London,Reading,Staines,Woki...</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Mead House</td>\n",
       "      <td>Sales Jobs</td>\n",
       "      <td>60k - 70k pa + OTE 100k + Car + Benefits</td>\n",
       "      <td>65000</td>\n",
       "      <td>jobsite.co.uk</td>\n",
       "      <td>11.082158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8078</th>\n",
       "      <td>65134317</td>\n",
       "      <td>Chef De Partie, Cambridge Centre, Fresh Modern...</td>\n",
       "      <td>Chef De Partie, Cambridge Centre, Fresh Modern...</td>\n",
       "      <td>Cambridgeshire, UK, Cambridgeshire</td>\n",
       "      <td>Cambridgeshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clear Selection</td>\n",
       "      <td>Hospitality &amp; Catering Jobs</td>\n",
       "      <td>18000 per annum</td>\n",
       "      <td>18000</td>\n",
       "      <td>jobs.catererandhotelkeeper.com</td>\n",
       "      <td>9.798182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                                              Title  \\\n",
       "15485   66750798                                 Accounts Assistant   \n",
       "241558  72678055               UK Country Manager / Sales Executive   \n",
       "8078    65134317  Chef De Partie, Cambridge Centre, Fresh Modern...   \n",
       "\n",
       "                                          FullDescription  \\\n",
       "15485   Accounts Assistant **** **** Hereford A Herefo...   \n",
       "241558  Job Title UK Country Manager / Sales Executive...   \n",
       "8078    Chef De Partie, Cambridge Centre, Fresh Modern...   \n",
       "\n",
       "                                              LocationRaw LocationNormalized  \\\n",
       "15485                                            Hereford           Hereford   \n",
       "241558  South East England,London,Reading,Staines,Woki...             London   \n",
       "8078                   Cambridgeshire, UK, Cambridgeshire     Cambridgeshire   \n",
       "\n",
       "       ContractType ContractTime          Company  \\\n",
       "15485           NaN    permanent              NaN   \n",
       "241558          NaN    permanent       Mead House   \n",
       "8078            NaN          NaN  Clear Selection   \n",
       "\n",
       "                           Category                                 SalaryRaw  \\\n",
       "15485     Accounting & Finance Jobs                           15,000 - 20,000   \n",
       "241558                   Sales Jobs  60k - 70k pa + OTE 100k + Car + Benefits   \n",
       "8078    Hospitality & Catering Jobs                           18000 per annum   \n",
       "\n",
       "        SalaryNormalized                      SourceName  Log1pSalary  \n",
       "15485              17500                      hays.co.uk     9.770013  \n",
       "241558             65000                   jobsite.co.uk    11.082158  \n",
       "8078               18000  jobs.catererandhotelkeeper.com     9.798182  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
    "TARGET_COLUMN = \"Log1pSalary\"\n",
    "\n",
    "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast missing values to string \"NaN\"\n",
    "\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title\n",
      "FullDescription\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "for text_column in text_columns:\n",
    "    print(text_column)\n",
    "    data[text_column] = [' '.join(tokenizer.tokenize(str(full_description).lower())) for full_description in data[text_column]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating through data rows: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 244768/244768 [00:00<00:00, 1098708.90it/s]\n",
      "Iterating through data rows: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 244768/244768 [00:10<00:00, 24168.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm, trange\n",
    "token_counts = Counter()\n",
    "\n",
    "# Count how many times does each token occur in both \"Title\" and \"FullDescription\" in total\n",
    "for text_column in text_columns:\n",
    "    for text in tqdm(data[text_column], \"Iterating through data rows\"):\n",
    "        for token in text.split():\n",
    "            token_counts[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "# tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
    "tokens = sorted(t for t, c in token_counts.items() if c >= min_count)#TODO<YOUR CODE HERE>\n",
    "\n",
    "# Add a special tokens for unknown and empty words\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34158/34158 [00:00<00:00, 1763181.01it/s]\n"
     ]
    }
   ],
   "source": [
    "token_to_id = dict() # <your code here - dict of token name to its index in tokens>\n",
    "for i in trange(len(tokens)):\n",
    "    token_to_id[tokens[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "        \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DictVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;, sparse=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DictVectorizer</label><div class=\"sk-toggleable__content\"><pre>DictVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;, sparse=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float32'>, sparse=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# we only consider top-1k most frequent companies to minimize memory usage\n",
    "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
    "recognized_companies = set(top_companies)\n",
    "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "\n",
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  195814\n",
      "Validation size =  48954\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "data_train.index = range(len(data_train))\n",
    "data_val.index = range(len(data_val))\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def to_tensors(batch, device):\n",
    "    batch_tensors = dict()\n",
    "    for key, arr in batch.items():\n",
    "        if key in [\"FullDescription\", \"Title\"]:\n",
    "            batch_tensors[key] = torch.tensor(arr, device=device, dtype=torch.int64)\n",
    "        else:\n",
    "            batch_tensors[key] = torch.tensor(arr, device=device)\n",
    "    return batch_tensors\n",
    "\n",
    "\n",
    "def make_batch(data, max_len=None, word_dropout=0, device=device):\n",
    "    \"\"\"\n",
    "    Creates a keras-friendly dict from the batch data.\n",
    "    :param word_dropout: replaces token index with UNK_IX with this probability\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    batch[\"Title\"] = as_matrix(data[\"Title\"].values, max_len)\n",
    "    batch[\"FullDescription\"] = as_matrix(data[\"FullDescription\"].values, max_len)\n",
    "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
    "    \n",
    "    if word_dropout != 0:\n",
    "        batch[\"FullDescription\"] = apply_word_dropout(batch[\"FullDescription\"], 1. - word_dropout)\n",
    "    \n",
    "    if TARGET_COLUMN in data.columns:\n",
    "        batch[TARGET_COLUMN] = data[TARGET_COLUMN].values\n",
    "    \n",
    "    return to_tensors(batch, device)\n",
    "\n",
    "def apply_word_dropout(matrix, keep_prop, replace_with=UNK_IX, pad_ix=PAD_IX,):\n",
    "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1 - keep_prop])\n",
    "    dropout_mask &= matrix != pad_ix\n",
    "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': tensor([[27645, 29893, 33674,     1,     1,     1,     1],\n",
       "         [29239,   197, 19175, 20042, 15554, 23162,  4051],\n",
       "         [10609, 30412, 17746,    33,  8705, 29157,    65]]),\n",
       " 'FullDescription': tensor([[27645, 29893, 33674, 32939,   982, 27645, 29893, 33674, 16451, 32939],\n",
       "         [29239,   197, 19175, 20042, 15554, 23162,  4051, 25511,   907,    82],\n",
       "         [30746, 21956, 20601,  6409, 16451,  8165, 27493,   982, 30412, 17746]]),\n",
       " 'Categorical': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'Log1pSalary': tensor([ 9.7115, 10.4631, 10.7144])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_batch(data_train[:3], max_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, hid_size=64):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(len(tokens), 32)\n",
    "        self.conv = torch.nn.Conv1d(32, hid_size, kernel_size=3, )\n",
    "\n",
    "    def forward(self, idx):\n",
    "        embs = self.emb(idx)\n",
    "        embs = embs.transpose(2, 1)\n",
    "        conv_outs = self.conv(embs)\n",
    "        max_pool = torch.max(conv_outs, dim=-1).values\n",
    "        return max_pool\n",
    "    \n",
    "class SalaryPredictor(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_), hid_size=64):\n",
    "        super().__init__()\n",
    "        self.title_enc = TextEncoder(hid_size=hid_size)\n",
    "        self.descr_enc = TextEncoder(hid_size=hid_size)\n",
    "        self.title_enc.emb = self.descr_enc.emb\n",
    "        self.categ_enc = nn.Linear(n_cat_features, hid_size)\n",
    "        self.output = nn.Linear(hid_size * 3, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        title = self.title_enc.forward(batch[\"Title\"])\n",
    "        descr = self.descr_enc.forward(batch[\"FullDescription\"])\n",
    "        categ = self.categ_enc.forward(batch[\"Categorical\"])\n",
    "        \n",
    "        concat = torch.concat((title.T, descr.T, categ.T)).T\n",
    "        return self.output.forward(concat).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SalaryPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(data, batch_size=256, shuffle=True, cycle=False, device=device, **kwargs):\n",
    "    \"\"\" iterates minibatches of data in random order \"\"\"\n",
    "    while True:\n",
    "        indices = np.arange(len(data))\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch = make_batch(data.iloc[indices[start : start + batch_size]], device=device, **kwargs)\n",
    "            yield batch\n",
    "        \n",
    "        if not cycle: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model, data, batch_size=BATCH_SIZE, name=\"\", device=torch.device('cpu'), **kw):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterate_minibatches(data, batch_size=batch_size, shuffle=False, device=device, **kw):\n",
    "            batch_pred = model(batch)\n",
    "            squared_error += torch.sum(torch.square(batch_pred - batch[TARGET_COLUMN]))\n",
    "            abs_error += torch.sum(torch.abs(batch_pred - batch[TARGET_COLUMN]))\n",
    "            num_samples += len(batch_pred)\n",
    "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
    "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
    "    print(\"%s results:\" % (name or \"\"))\n",
    "    print(\"Mean square error: %.5f\" % mse)\n",
    "    print(\"Mean absolute error: %.5f\" % mae)\n",
    "    return mse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ec4823c11a40c1a1009cad367ec8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.23696\n",
      "Mean absolute error: 0.39767\n"
     ]
    }
   ],
   "source": [
    "model = SalaryPredictor().to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(\n",
    "            iterate_minibatches(data_train, batch_size=BATCH_SIZE, device=device)),\n",
    "            total=len(data_train) // BATCH_SIZE\n",
    "        ):\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch[TARGET_COLUMN])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print_metrics(model, data_val, device=device)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pred = [5.495172  5.9892273 7.39333  ]\n",
      "test_true = [10.064798  10.045031  10.4588375]\n"
     ]
    }
   ],
   "source": [
    "test_batch = make_batch(data_val[:3], max_len=10)\n",
    "test_pred = model(test_batch)\n",
    "print(f\"test_pred = {test_pred.detach().cpu().numpy()}\")\n",
    "print(f\"test_true = {data_val['Log1pSalary'][:3].values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalaryPredictorExtraLayers(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_), hid_size=64, hid_size_2 = 32):\n",
    "        super().__init__()\n",
    "        self.title_enc = TextEncoder(hid_size=hid_size)\n",
    "        self.descr_enc = TextEncoder(hid_size=hid_size)\n",
    "        self.title_enc.emb = self.descr_enc.emb\n",
    "        self.categ_enc = nn.Linear(n_cat_features, hid_size)\n",
    "        self.linear = nn.Linear(hid_size * 3, hid_size_2)\n",
    "        self.output = nn.Linear(hid_size_2, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        title = self.title_enc.forward(batch[\"Title\"])\n",
    "        descr = self.descr_enc.forward(batch[\"FullDescription\"])\n",
    "        categ = self.categ_enc.forward(batch[\"Categorical\"])\n",
    "        \n",
    "        concat = torch.concat((title.T, descr.T, categ.T)).T\n",
    "        linear_result = self.linear.forward(concat)\n",
    "        return self.output.forward(linear_result).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0a7d2460794d2b8a0c3bfbbe91d139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.19252\n",
      "Mean absolute error: 0.35146\n"
     ]
    }
   ],
   "source": [
    "model = SalaryPredictorExtraLayers().to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(\n",
    "            iterate_minibatches(data_train, batch_size=BATCH_SIZE, device=device)),\n",
    "            total=len(data_train) // BATCH_SIZE\n",
    "        ):\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch[TARGET_COLUMN])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print_metrics(model, data_val, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoderFewConvs(nn.Module):\n",
    "    def __init__(self, hid_size=32):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(len(tokens), 32)\n",
    "        self.conv1 = torch.nn.Conv1d(32, hid_size, kernel_size=3, )\n",
    "        self.conv2 = torch.nn.Conv1d(32, hid_size, kernel_size=3, )\n",
    "\n",
    "    def forward(self, idx):\n",
    "        embs = self.emb(idx)\n",
    "        embs = embs.transpose(2, 1)\n",
    "        conv1_outs = self.conv1(embs)\n",
    "        conv2_outs = self.conv2(embs)\n",
    "        max_pool1 = torch.max(conv1_outs, dim=-1).values\n",
    "        max_pool2 = torch.max(conv2_outs, dim=-1).values\n",
    "        return torch.concat((max_pool1.T, max_pool2.T)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = make_batch(data_train[:3], max_len=10)\n",
    "enc = TextEncoderFewConvs()\n",
    "enc.forward(batch[\"Title\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalaryPredictorExtraLayersWithNewTextEncoders(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_), hid_size=64, hid_size_2 = 32):\n",
    "        super().__init__()\n",
    "        self.title_enc = TextEncoderFewConvs(hid_size=hid_size // 2)\n",
    "        self.descr_enc = TextEncoderFewConvs(hid_size=hid_size // 2)\n",
    "        self.title_enc.emb = self.descr_enc.emb\n",
    "        self.categ_enc = nn.Linear(n_cat_features, hid_size)\n",
    "        self.linear = nn.Linear(hid_size * 3, hid_size_2)\n",
    "        self.output = nn.Linear(hid_size_2, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        title = self.title_enc.forward(batch[\"Title\"])\n",
    "        descr = self.descr_enc.forward(batch[\"FullDescription\"])\n",
    "        categ = self.categ_enc.forward(batch[\"Categorical\"])\n",
    "        \n",
    "        concat = torch.concat((title.T, descr.T, categ.T)).T\n",
    "        linear_result = self.linear.forward(concat)\n",
    "        return self.output.forward(linear_result).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bdf6b41f3540359872b2083d205b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.17977\n",
      "Mean absolute error: 0.33695\n"
     ]
    }
   ],
   "source": [
    "model = SalaryPredictorExtraLayersWithNewTextEncoders().to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(\n",
    "            iterate_minibatches(data_train, batch_size=BATCH_SIZE, device=device)),\n",
    "            total=len(data_train) // BATCH_SIZE\n",
    "        ):\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch[TARGET_COLUMN])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print_metrics(model, data_val, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoderDifferentPullings(nn.Module):\n",
    "    def __init__(self, hid_size=32):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(len(tokens), 32)\n",
    "        self.conv = torch.nn.Conv1d(32, hid_size, kernel_size=3, )\n",
    "\n",
    "    def forward(self, idx):\n",
    "        embs = self.emb(idx)\n",
    "        embs = embs.transpose(2, 1)\n",
    "        conv_outs = self.conv(embs)\n",
    "        max_pool = torch.max(conv_outs, dim=-1).values\n",
    "        min_pool = torch.min(conv_outs, dim=-1).values\n",
    "        return torch.concat((max_pool.T, min_pool.T)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = make_batch(data_train[:3], max_len=10)\n",
    "enc = TextEncoderDifferentPullings()\n",
    "enc.forward(batch[\"Title\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalaryPredictorExtraLayersWithNew2TextEncoders(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_), hid_size=64, hid_size_2 = 32):\n",
    "        super().__init__()\n",
    "        self.title_enc = TextEncoderDifferentPullings(hid_size=hid_size // 2)\n",
    "        self.descr_enc = TextEncoderDifferentPullings(hid_size=hid_size // 2)\n",
    "        self.title_enc.emb = self.descr_enc.emb\n",
    "        self.categ_enc = nn.Linear(n_cat_features, hid_size)\n",
    "        self.linear = nn.Linear(hid_size * 3, hid_size_2)\n",
    "        self.output = nn.Linear(hid_size_2, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        title = self.title_enc.forward(batch[\"Title\"])\n",
    "        descr = self.descr_enc.forward(batch[\"FullDescription\"])\n",
    "        categ = self.categ_enc.forward(batch[\"Categorical\"])\n",
    "        \n",
    "        concat = torch.concat((title.T, descr.T, categ.T)).T\n",
    "        linear_result = self.linear.forward(concat)\n",
    "        return self.output.forward(linear_result).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cbbac3aac44bdeb437023910e0f765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " results:\n",
      "Mean square error: 0.15767\n",
      "Mean absolute error: 0.30526\n"
     ]
    }
   ],
   "source": [
    "model = SalaryPredictorExtraLayersWithNew2TextEncoders().to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(\n",
    "            iterate_minibatches(data_train, batch_size=BATCH_SIZE, device=device)),\n",
    "            total=len(data_train) // BATCH_SIZE\n",
    "        ):\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch[TARGET_COLUMN])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print_metrics(model, data_val, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short report\n",
    "\n",
    "Please tell us what you did and how did it work.\n",
    "\n",
    "Baseline learned for 5 epochs gives a `MSE = 0.115, MAE = 0.265`\n",
    "\n",
    "For reducing learning time lets learn only 1 epoch. (I think results can be scaled to N epochs with approximately same effects)\n",
    "\n",
    "Baseline losses on 1 epoch are `MSE = 0.237, MAE = 0.398`\n",
    "\n",
    "Let's view how CNN architecture will affect losses.\n",
    "\n",
    "So, extra linear layer with hidden_size 32 gives us a significant boost: `MSE = 0.193, MAE = 0.351`.\n",
    "\n",
    "Parallel convolution layers gives us a visible boost as well: `MSE = 0.180, MAE = 0.337` (comparing to previous model).\n",
    "\n",
    "\n",
    "Now let's rollback to just extra layer and try to play with pullings. Now we will make max & min pullings.\n",
    "\n",
    "We have incredible results: `MSE = 0.158, MAE = 0.305`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use early stopping to learn out model well. Source: https://pythonguides.com/pytorch-early-stopping/\n",
    "\n",
    "Could not debut it :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch import nn\n",
    "from torch.nn import functional as f\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "PATHDATASET = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAILGPUS = min(1, torch.cuda.device_count())\n",
    "BATCHSIZE = 250 if AVAILGPUS else 60\n",
    "    \n",
    "class SalaryPredictorExtraLayersWithNewTextEncodersLightning(LightningModule):\n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_), hid_size=64, hid_size_2 = 32):\n",
    "        super().__init__()\n",
    "        self.title_enc = TextEncoderFewConvs(hid_size=hid_size // 2)\n",
    "        self.descr_enc = TextEncoderFewConvs(hid_size=hid_size // 2)\n",
    "        self.title_enc.emb = self.descr_enc.emb\n",
    "        self.categ_enc = nn.Linear(n_cat_features, hid_size)\n",
    "        self.linear = nn.Linear(hid_size * 3, hid_size_2)\n",
    "        self.output = nn.Linear(hid_size_2, 1)\n",
    "        self.criterion = nn.MSELoss(reduction='sum')\n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        title = self.title_enc.forward(batch[\"Title\"])\n",
    "        descr = self.descr_enc.forward(batch[\"FullDescription\"])\n",
    "        categ = self.categ_enc.forward(batch[\"Categorical\"])\n",
    "        \n",
    "        concat = torch.concat((title.T, descr.T, categ.T)).T\n",
    "        linear_result = self.linear.forward(concat)\n",
    "        return self.output.forward(linear_result).flatten()\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        X, y = batch\n",
    "        loss = self.criterion(self(X), y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type                | Params\n",
      "--------------------------------------------------\n",
      "0 | title_enc | TextEncoderFewConvs | 1.1 M \n",
      "1 | descr_enc | TextEncoderFewConvs | 1.1 M \n",
      "2 | categ_enc | Linear              | 241 K \n",
      "3 | linear    | Linear              | 6.2 K \n",
      "4 | output    | Linear              | 33    \n",
      "5 | criterion | MSELoss             | 0     \n",
      "--------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.412     Total estimated model params size (MB)\n",
      "/Users/nvdenisov2002/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Error: command buffer exited with error status.\n",
      "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
      "\tError: \n",
      "\t(null)\n",
      "\tInsufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)\n",
      "\t<AGXG13XFamilyCommandBuffer: 0x28d3701d0>\n",
      "    label = <none> \n",
      "    device = <AGXG13XDevice: 0x2d4a2aa00>\n",
      "        name = Apple M1 Pro \n",
      "    commandQueue = <AGXG13XFamilyCommandQueue: 0x2d4c99000>\n",
      "        label = <none> \n",
      "        device = <AGXG13XDevice: 0x2d4a2aa00>\n",
      "            name = Apple M1 Pro \n",
      "    retainedReferences = 1\n",
      "Error: command buffer exited with error status.\n",
      "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
      "\tError: \n",
      "\t(null)\n",
      "\tInsufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)\n",
      "\t<AGXG13XFamilyCommandBuffer: 0x2b71fef40>\n",
      "    label = <none> \n",
      "    device = <AGXG13XDevice: 0x2d4a2aa00>\n",
      "        name = Apple M1 Pro \n",
      "    commandQueue = <AGXG13XFamilyCommandQueue: 0x2d4c99000>\n",
      "        label = <none> \n",
      "        device = <AGXG13XDevice: 0x2d4a2aa00>\n",
      "            name = Apple M1 Pro \n",
      "    retainedReferences = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c810f9b0c1246a29b834b08799c962b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     10\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     progress_bar_refresh_rate=20,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model \u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(mnistmodel, train_loader)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    534\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:355\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:219\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautomatic_optimization\u001b[38;5;241m.\u001b[39mrun(trainer\u001b[38;5;241m.\u001b[39moptimizers[\u001b[38;5;241m0\u001b[39m], kwargs)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:188\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         closure()\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m), closure)\n\u001b[1;32m    190\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:266\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\n\u001b[1;32m    267\u001b[0m     trainer,\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_step\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    269\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mcurrent_epoch,\n\u001b[1;32m    270\u001b[0m     batch_idx,\n\u001b[1;32m    271\u001b[0m     optimizer,\n\u001b[1;32m    272\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:146\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 146\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    149\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1276\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1240\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1243\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1244\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer`\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;124;03m    calls the optimizer.\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;124;03m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1276\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39moptimizer_closure)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:161\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:231\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:116\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 143\u001b[0m         loss \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    146\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:103\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:142\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:128\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 128\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_fn()\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:294\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 294\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    297\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:380\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, TrainingStep)\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[50], line 39\u001b[0m, in \u001b[0;36mSalaryPredictorExtraLayersWithNewTextEncodersLightning.training_step\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_nb):\n\u001b[1;32m     38\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 39\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(\u001b[38;5;28mself\u001b[39m(X), y)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/shad/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 29\u001b[0m, in \u001b[0;36mSalaryPredictorExtraLayersWithNewTextEncodersLightning.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m---> 29\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtitle_enc\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     30\u001b[0m     descr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescr_enc\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFullDescription\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     31\u001b[0m     categ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcateg_enc\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategorical\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 4"
     ]
    }
   ],
   "source": [
    "# Init our model\n",
    "mnistmodel = model = SalaryPredictorExtraLayersWithNewTextEncodersLightning()\n",
    "\n",
    "# Init DataLoader from MNIST Dataset\n",
    "traindataset = MNIST(\".\", train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(traindataset, batch_size=BATCHSIZE)\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "#     progress_bar_refresh_rate=20,\n",
    ")\n",
    "\n",
    "# Train the model \n",
    "trainer.fit(mnistmodel, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended options\n",
    "\n",
    "#### A) CNN architecture\n",
    "\n",
    "All the tricks you know about dense and convolutional neural networks apply here as well.\n",
    "* Dropout. Nuff said.\n",
    "* Batch Norm. This time it's `nn.BatchNorm*`/`L.BatchNormalization`\n",
    "* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels.\n",
    "* More layers, more neurons, ya know...\n",
    "\n",
    "\n",
    "#### B) Play with pooling\n",
    "\n",
    "There's more than one way to perform pooling:\n",
    "* Max over time (independently for each feature)\n",
    "* Average over time (excluding PAD)\n",
    "* Softmax-pooling:\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n",
    "\n",
    "* Attentive pooling\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$\n",
    "\n",
    ", where $$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n",
    "and $NN_{attn}$ is a dense layer.\n",
    "\n",
    "The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$ (aka multi-headed attention).\n",
    "\n",
    "The catch is that keras layers do not inlude those toys. You will have to [write your own keras layer](https://keras.io/layers/writing-your-own-keras-layers/). Or use pure tensorflow, it might even be easier :)\n",
    "\n",
    "#### C) Fun with words\n",
    "\n",
    "It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n",
    "\n",
    "* Use a pre-trained embeddings from `gensim.downloader.load`. See last lecture.\n",
    "* Start with pre-trained embeddings, then fine-tune them with gradient descent. You may or may not download pre-trained embeddings from [here](http://nlp.stanford.edu/data/glove.6B.zip) and follow this [manual](https://keras.io/examples/nlp/pretrained_word_embeddings/) to initialize your Keras embedding layer with downloaded weights.\n",
    "* Use the same embedding matrix in title and desc vectorizer\n",
    "\n",
    "\n",
    "#### D) Going recurrent\n",
    "\n",
    "We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n",
    "\n",
    "* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n",
    "* Since you know all the text in advance, use bidirectional RNN\n",
    "  * Run one LSTM from left to right\n",
    "  * Run another in parallel from right to left \n",
    "  * Concatenate their output sequences along unit axis (dim=-1)\n",
    "\n",
    "* It might be good idea to mix convolutions and recurrent layers differently for title and description\n",
    "\n",
    "\n",
    "#### E) Optimizing seriously\n",
    "\n",
    "* You don't necessarily need 100 epochs. Use early stopping. If you've never done this before, take a look at [early stopping callback(keras)](https://keras.io/callbacks/#earlystopping) or in [pytorch(lightning)](https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html).\n",
    "  * In short, train until you notice that validation\n",
    "  * Maintain the best-on-validation snapshot via `model.save(file_name)`\n",
    "  * Plotting learning curves is usually a good idea\n",
    "  \n",
    "Good luck! And may the force be with you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
